#Chpater3
###类别不平衡问题(class imbalance)
####类别不平衡(class imbalance)
#Chapter4 决策树
##4.1 基本流程
####决策树基于树结构来进行预测
-    如果用决策树来进行分来,起码该模型一定意义上是可以理解的
####树结构的return
#####(1)当前节点包含的样本全部属于同一类别(没必要分类)
#####(2)当前的属性集为空,或所有样本所有属性上取值相同(没法分类)
#####(3)当前节点包含的样本集合为空($ \emptyset $)
####划分选择
-   希望决策树的分支节点包含的样本尽可能属于同一类别,即节点的"纯度"(purity)越来越高

-   经典的属性划分方法:
    1)信息增益,
    2)增益率,
    3)基尼指数
#####划分选择-信息增益
-   “信息熵”是度量样本集合纯度最常用的一种指标
######信息熵
-   $$Ent(D)=-\sum_{k=1}^{|y|}p_klog_2p_k$$
-   推导:$$\int P(x)f(x)dx\rightarrow E_{x - p}(f(x))\rightarrow E_{x-p} (log_2 \frac{1}{p_k})$$
    +   (log2可以表示用二进制表示,$\frac{1}{p_k}$可以显示信息(概率越低越刺激))
    +   计算信息熵的约定:若p=0,则Ent=0
    +   Ent(D)的值越小,纯度越大
######信息增益
-   $$Gain(D,a)=Ent(D)-\sum_{v=1}^{V}\frac{|D^{v}|}{|D|}Ent(D^{v})$$
-   算出信息增益之后,可以确定树的每一层应该对应哪些划分属性


