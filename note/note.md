# Chapter 2

##空间

- 假设空间
  -  假设满足XX条件的是好瓜

- 版本空间
  - 有限训练集，已知XX是好瓜

- 归纳偏好
  - 假设空间和训练集一致的假设
  - 学习过程中对某种类型假设的偏好称为归纳偏好
- No Free Lunch
  - 奥卡姆剃刀：两个模型效果同样好，选择较为简单的

## 模型评估与选择

- 经验误差与过拟合
  - 错误率率&误差
    - 错误率：错份样本的占$E=a/m$
    - 误差：样本真实输出与预测输出之间的差异
      - 训练（经验）误差：训练集上
      - 测试误差：测试集
      - 泛化误差：初训练集外所有样本
- 过拟合
  - 学习器把训练样本学习的“太好”，将训练样本本身的特点当作所有样本的一般性质，导致泛化性能下降
  - 优化目标加正则项
  - Early stop
- 欠拟合
  - 对训练样本的一般性质尚未学好
  - 决策树：扩展分支
  - 神经网络：增加训练层数
- 评估方法
  - 留出法
    - 直接将数据集划分为两个互斥集合
    - 训练/测试集划分要尽可能保持数据分布的一致性
    - 一般若干次随机划分，重复实验取平均值
    - 训练/测试样本比例通常为2:1～4:1
  - 交叉验证法
    - 将数据集分层采样划分为$k$个大小相似的互斥子集
  - 自助法
    - 以自助采样法为基础，对数据集$D$有放回采样$m$次得到训练集$D^{\prime}$，$D\backslash D^{\prime}$用作测试集
- 性能度量
  - 性能度量是衡量模型泛化能力的评价标准，反映任务的需求
    - 回归任务最常用的是“均方误差”：
      - $E(f:D)=\frac{1}{m}\sum_{i=1}^{m}(f(x_i)-y_i)^{2}$
  - 查准率 $P=\frac{TP}{TP+FP}$
  - 查全率 $R=\frac{TP}{TP+FN}$
  - $P-R$曲线
  - $F1$ measure：$\frac{2\times TP}{N+TP-TN}$
  - $AUC=\frac{1}{2}\sum_{i=1}^{m-1}(x_{i+1}-x_{i})\cdot(y_{i}+y_{i+1})$，预测了排序质量
  - 代价敏感错误率
- 性能评估
  - 关于性能比较
    - 测试性能并不等于泛化性能
    - 测试性能随着测试集的变化而变化
    - 很多机器学习算法本身有一定的随机性
    - 直接选取相应评估方式在相应条件下评估并不可靠
  - 二项检验
    - 泛化错误率为$\epsilon$，测试错误率为$\hat{\epsilon}$，嘉定测试样本从样本总体分布中独立采样而来，我们可以使用“二项检验”，对于$\epsilon<epsilon_{0}$进行假设检验。
    - 假设$\epsilon\leq\epsilon_{0}$，若测试错误率小于
  - $t$检验
  - 交叉验证$t$检验
- 偏差和方差
  - 对于测试样本$x$，令$y_{D}$为$x$在数据集中的标记，$y$为$x$的真实标记，$f(x;D)$为训练集$D$上学的模型$f$在$x$上的预测输出。
  - 以回归任务为例：
    - 期望预期为：$\bar{f}(x)=\mathbb{E}_{D}[f(x;D)]$；
    - 使用样本数目相同的不同训练集产生的方差为$var(x)=\mathbb{E}_{D}[(f(x:D)-\bar{f}(x))^{2}]$；
    - 噪声为$\varepsilon^{2}=\mathbb{E}_{D}[(y_{D}-y)^{2}]​$
  - $E(f;D)=bias^{2}(x)+var(x)+\varepsilon^2$

#Chapter 3

##基本形式

- 线性模型一般形式$f(x)=w_1x_1+w_2x_2+\cdots+w_dx_d+b$
- 向量形式$f(x)=w^{T}x+b​$

- 区分猫狗的例子
  - 按照像素行堆叠或列堆叠，成为一个向量
  - 乘以单位，对于二分类问题，单位就是一个向量
- Perceptron感知机
  - 对于线性分类器，误分类则$-y_{1}(w\cdot x_i)+b>0$
  - 定义损失函数$L(w,b)=-\sum_{x_{i}\in M}y_{i}(w\cdot x_{i}+b)$
  - 梯度$\bigtriangledown_{w}L(w,b)=-\sum_{x_{i}\in M}y_{i}x_{i}​$
  - $\bigtriangledown_{b}L(w,b)=-\sum_{x_{i}\in M}y_{i}​$
- 梯度下降法
  - 一阶方法
  - 考虑无约束优化$min_{x}f(x)$，$f(x+\Delta x)\approx f(x)+\Delta x^{T}\bigtriangledown f(x)$
  - $\Delta x^{T}\bigtriangledown f(x)<0​$
  - $\Delta x=-\gamma \bigtriangledown f(x)$
  - $\gamma$使用二分查找法进行查找
- 优点
  - 形式简单，易于建模
  - 可解释性
  - 非线性模型的基础
    - 引入层级结构或高维映射
- 缺陷
  - 解决不了$x^{2}$问题

## 线性回归

- 目的：学得一个线性模型以尽可能准确地预测实值输出标记

- 离散属性处理
  - 有“序”关系
    - 连续化为连续值
  - 无“序”关系
- 单一属性的线性回归目标
  - $f(x)=wx_{i}+b$使得$f(x_{i})\simeq y_{i}$
- 参数/模型估计：最小二乘法（Least square method）
  - $(w^{*},b^{*})=arg\ min_{(w,b)}\sum_{i=1}^{m}(f(x_{i})-y_i)^2​$
  - 最小化均方误差$E_{(w,b)}$

- 多元线性回归
  - $f(\hat{x_{i}})=\hat{x_{i}}^{T}(X^{T}X)^{-1}$
  - $X^{T}X$不满秩，进行正则化
- 广义线性模型
  - 一般形式：$y=g^{-1}(w^{T}x+b)$
    - $g$为联系函数(link function)

## 二分类问题

- 预测值与输出标记$z=w^{T}x+b$
- 寻找函数将分类标记与线性回归模型输出联系起来
- 最理想的模型——单位阶跃函数
- 替代函数——对数几率函数（logistic function）
  - **$y=\frac{1}{1+e^{-z}}$**
  - 运行对数几率函数$y=\frac{1}{1+e^{-z}}=\frac{1}{1+e^{-(w^{T}x+b)}}$
  - 对数几率

- 样本作为正例的相对可能性的对数$\ln\frac{y}{1-y}=\ln\frac{p(y=1|x)}{p(y=0|x)}=w^{T}x+b$

- 极大似然法

- 给定数据集$\{(x_{i}, y_{i})\}^{m}_{i=1}$
- 最大化样本属于其真实标记的概率

- 线性判别分析（Linear Discriminant Analysis）
  - 最大化目标$J=\frac{\|w^{T}\mu_{0}-w^{T}\mu_{1}\|^{2}_{2}}{w^{T}\sum_{0}w+w^{T}\sum_{1}w}=\frac{w^{T}(\mu_{0}-\mu_{1})(\mu_{0}-\mu_{1})^{T}w}{w^{T}(\sum_{0}+\sum_{1})w}​$
  - 类间散度矩阵，类内散度矩阵
  - 广义瑞丽商$J=\frac{w^{T}S_{b}w}{w^{T}S_{w}w}$
  - 拉格朗日乘子法
    - $\bigtriangledown f(x^{*})+\lambda \Delta g(x^{*})=0$
    - $L(x,\lambda)=f(x)+\lambda g(x)$.

## 多分类问题

- 多分类学习方法
  - 二分类学习方法推广到多类
  - 利用二分类学习器解决多分类问题
    - 对问题进行拆分，为拆出的每个二分类任务训练一个分类器
    - 对于每个分类器的预测结果进行集成以获得最终的多分类结果
  - 拆分策略
    - 一对一（OVO）
      - $N$个类别两两配对，$N(N-1)/2$个二类任务
      - 各个二类任务学习分类器，$N(N-1)/2$个二类分类器
    - 一对其余（OVR）
    - 多对多（MVM）
      - 若干类作为正类，若干类作为反类
      - 输出纠错码（Error Correcting Output Code, ECOC）
  - 类别不平衡问题$(class\ imbalance)$
    - 不同类别训练样例数相差很大情况（正类为小类）
    - 类别平衡正例预测$\frac{y}{1-y}>1\Rightarrow \frac{y}{1-y}>\frac{m^{+}}{m^{-}}$正负类比例
  - 再缩放
    - 欠采样$(undersampling)$
      - 去除一些反例使正反例数目接近
    - 过采样$(oversampling)$
      - 增加一些正例使正反例数目接近
    - 阈值移动$(threshold-moving)$

#Chapter4 决策树
##4.1 基本流程
####决策树基于树结构来进行预测
-    如果用决策树来进行分来,起码该模型一定意义上是可以理解的
####树结构的return
#####(1)当前节点包含的样本全部属于同一类别(没必要分类)
#####(2)当前的属性集为空,或所有样本所有属性上取值相同(没法分类)
#####(3)当前节点包含的样本集合为空($ \emptyset $)
####划分选择
-   希望决策树的分支节点包含的样本尽可能属于同一类别,即节点的"纯度"(purity)越来越高

-   经典的属性划分方法:
    1)信息增益,
    2)增益率,
    3)基尼指数
#####划分选择-信息增益
-   “信息熵”是度量样本集合纯度最常用的一种指标
######信息熵
-   $$Ent(D)=-\sum_{k=1}^{|y|}p_klog_2p_k$$
-   推导:$$\int P(x)f(x)dx\rightarrow E_{x - p}(f(x))\rightarrow E_{x-p} (log_2 \frac{1}{p_k})$$
    +   (log2可以表示用二进制表示,$\frac{1}{p_k}$可以显示信息(概率越低越刺激))
    +   计算信息熵的约定:若p=0,则Ent=0
    +   Ent(D)的值越小,纯度越大
######信息增益
-   $$Gain(D,a)=Ent(D)-\sum_{v=1}^{V}\frac{|D^{v}|}{|D|}Ent(D^{v})$$
-   算出信息增益之后,可以确定树的每一层应该对应哪些划分属性
######存在的问题
-   信息增益对可取值数目较多的属性有所偏好
#####划分选择-增益率
-   增益率：$$Gain\_ ratio (D,a)=\frac{Gain(D,a)}{IV(a)}$$
    其中    $$IV(a)=-\sum _{v=1}^{V}\frac{D^v}{D}log_2 \frac{D^v}{D}$$
######存在的问题
-   增益率准则对可取值数目较少的属性有所偏好
#####划分选择-基尼指数
